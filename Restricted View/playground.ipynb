{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   2   0   2   1   0   0   0   2   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0 255   0   0   0   0   0   0\n",
      "Agent Position: 5 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   2   0   0   0   0   2   0\n",
      "  0   2   0   2   1   0   0   0   2   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0 255   0   0   0   0   0   0\n",
      "Agent Position: 5 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   2   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   2   0   0   0   0   2   0\n",
      "  0   2   0   2   1   0   0   0   2   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0 255   0   0   0   0   0   0   0\n",
      "Agent Position: 4 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  0   0   2   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   2   0   0   0   0   2   0\n",
      "  0   2   0   2   1   0   0   0   2   0   0   0\n",
      "  0   0   0   0   0 255   0   0   0   0   0   0\n",
      "Agent Position: 5 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  1   1   0   2   2   0   0   0   0   1   0   0\n",
      "  0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  0   0   2   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   2   0   0   0   0   2   0\n",
      "  0   0   0   0   0 255   0   0   0   0   0   0\n",
      "Agent Position: 5 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   2   0   2   0   0   0   2   2   0   0\n",
      "  1   1   0   2   2   0   0   0   0   1   0   0\n",
      "  0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  0   0   2   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0 255   0   0   0   0   0\n",
      "Agent Position: 6 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  1   0   0   1   0   0   0   0   1   0   0   2\n",
      "  0   0   2   0   2   0   0   0   2   2   0   0\n",
      "  1   1   0   2   2   0   0   0   0   1   0   0\n",
      "  0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0 255   0   0   0   0\n",
      "Agent Position: 7 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   1   0   0   0   0\n",
      "  1   0   0   1   0   0   0   0   1   0   0   2\n",
      "  0   0   2   0   2   0   0   0   2   2   0   0\n",
      "  1   1   0   2   2   0   0   0   0   1   0   0\n",
      "  0   0   0   0   0   0   0   0 255   0   0   0\n",
      "Agent Position: 8 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  2   0   0   0   0   0   0   0   0   2   0   0\n",
      "  0   0   0   0   0   0   0   1   0   0   0   0\n",
      "  1   0   0   1   0   0   0   0   1   0   0   2\n",
      "  0   0   2   0   2   0   0   0   2   2   0   0\n",
      "  0   0   0   0   0   0   0 255   0   0   0   0\n",
      "Agent Position: 7 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   1   0\n",
      "  2   0   0   0   0   0   0   0   0   2   0   0\n",
      "  0   0   0   0   0   0   0   1   0   0   0   0\n",
      "  1   0   0   1   0   0   0   0   1   0   0   2\n",
      "  0   0   0   0   0   0   0 255   0   0   0   0\n",
      "Agent Position: 7 | Score: 0\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   2   0   0   2   0   0   0   2   0\n",
      "  0   0   0   0   0   0   0   0   0   0   1   0\n",
      "  2   0   0   0   0   0   0   0   0   2   0   0\n",
      "  0   0   0   0   0   0   0   1   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0 255   0   0   0\n",
      "Agent Position: 8 | Score: 10\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   1   0   0   1   0   1   0   0   2   0\n",
      "  0   0   0   2   0   0   2   0   0   0   2   0\n",
      "  0   0   0   0   0   0   0   0   0   0   1   0\n",
      "  2   0   0   0   0   0   0   0   0   2   0   0\n",
      "  0   0   0   0   0   0   0   0   0 255   0   0\n",
      "Agent Position: 9 | Score: 10\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   2   0   0   0   0   0   0   0   0\n",
      "  0   0   1   0   0   1   0   1   0   0   2   0\n",
      "  0   0   0   2   0   0   2   0   0   0   2   0\n",
      "  0   0   0   0   0   0   0   0   0   0   1   0\n",
      "  0   0   0   0   0   0   0   0 255   0   0   0\n",
      "Agent Position: 8 | Score: 10\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   2   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   2   0   0   0   0   0   0   0   0\n",
      "  0   0   1   0   0   1   0   1   0   0   2   0\n",
      "  0   0   0   2   0   0   2   0   0   0   2   0\n",
      "  0   0   0   0   0   0   0   0   0 255   0   0\n",
      "Agent Position: 9 | Score: 10\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   2\n",
      "  0   2   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   2   0   0   0   0   0   0   0   0\n",
      "  0   0   1   0   0   1   0   1   0   0   2   0\n",
      "  0   0   0   0   0   0   0   0 255   0   0   0\n",
      "Agent Position: 8 | Score: 10\n",
      "\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   1   0   2   2   2   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   2\n",
      "  0   2   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   2   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0 255   0   0   0   0\n",
      "Agent Position: 7 | Score: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from typing import Tuple\n",
    "\n",
    "class BallCatchingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size: Tuple[int, int]=(5, 12), max_balls: int=7):\n",
    "        super(BallCatchingEnv, self).__init__()\n",
    "        self.grid_height, self.grid_width = grid_size\n",
    "        self.max_balls = max_balls\n",
    "\n",
    "        # Actions: 0 = Stay, 1 = Left, 2 = Right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation space: grid with values representing empty space, agent, and balls\n",
    "        # Let's use 0 for empty, 1 for ball, and a unique value (e.g., 255) for the agent\n",
    "        self.observation_space = spaces.Box(low=0, high=255, \n",
    "                                            shape=(self.grid_height, self.grid_width), \n",
    "                                            dtype=np.uint8)\n",
    "\n",
    "        self.agent_pos = self.grid_width // 2  # Start the agent in the middle of the grid\n",
    "        self.score = 0\n",
    "        self.caught_balls = 0\n",
    "        self.grid = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.grid_width // 2\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "        self.balls = []  # To track balls store them as a list of tuples (row, col)\n",
    "        self.score = 0\n",
    "        self.caught_balls = 0\n",
    "        # Place the agent on the grid\n",
    "        self.grid[-1, self.agent_pos] = 255  # Use the bottom row for the agent\n",
    "        return self.grid.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Ensure action is valid\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        # Move agent based on action\n",
    "        if action == 1 and self.agent_pos > 0:  # Move left\n",
    "            self.agent_pos -= 1\n",
    "        elif action == 2 and self.agent_pos < self.grid_width - 1:  # Move right\n",
    "            self.agent_pos += 1\n",
    "        # Note: action == 0 means stay, so no movement code is needed for that case\n",
    "\n",
    "        # Generate new balls at the top of the grid\n",
    "        num_new_balls = np.random.randint(1, self.max_balls + 1)\n",
    "        for _ in range(num_new_balls):\n",
    "            ball_col = np.random.randint(0, self.grid_width)\n",
    "            ball_type = np.random.choice([1, 2])  # 1 for red, 2 for blue\n",
    "            self.balls.append([0, ball_col, ball_type])  # Append new ball at top row with random column\n",
    "\n",
    "        # Move existing balls down one row and prepare for checking catches\n",
    "        \"\"\"\n",
    "        balls_to_remove = []\n",
    "        for ball in self.balls:\n",
    "            ball[0] += 1  # Move ball down one row\n",
    "            if ball[0] == self.grid_height - 1:  # Ball is exactly at the bottom row\n",
    "                if ball[1] == self.agent_pos:  # And in the same column as the agent\n",
    "                    print(f\"Catching ball of type: {ball[2]}, Current Score: {self.score}\")  # Debugging output\n",
    "                    self.caught_balls += 1\n",
    "                    self.score += 20 if ball[2] == 2 else 10  # Update score based on ball type\n",
    "                    print(f\"New Score: {self.score}\")  # Debugging output\n",
    "\n",
    "        # Remove balls that have reached the bottom or been caught\n",
    "        for ball in balls_to_remove:\n",
    "            self.balls.remove(ball)\n",
    "\n",
    "        \"\"\"\n",
    "        # Move existing balls down one row and check for catches\n",
    "        for ball in list(self.balls):  # Safe iteration over a copy of the list\n",
    "            ball[0] += 1  # Move ball down one row\n",
    "            if ball[0] == self.grid_height - 1:  # If the ball is exactly at the bottom row\n",
    "                if ball[1] == self.agent_pos:  # And in the same column as the agent\n",
    "                    #print(f\"Catching ball of type: {ball[2]}, Current Score: {self.score}\") \n",
    "                    self.caught_balls += 1\n",
    "                    self.score += 20 if ball[2] == 2 else 10  # Score based on ball type\n",
    "                    #print(f\"New Score: {self.score}\") \n",
    "                # Remove the ball after processing, whether caught or missed\n",
    "                self.balls.remove(ball)\n",
    "        \n",
    "\n",
    "        # Update grid representation\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "        for ball in self.balls:\n",
    "            self.grid[ball[0], ball[1]] = ball[2]  # Place balls on the grid\n",
    "        self.grid[self.grid_height - 1, self.agent_pos] = 255  # Place the agent on the grid\n",
    "\n",
    "\n",
    "        # Check for episode termination\n",
    "        done = self.caught_balls >= 2\n",
    "\n",
    "        # info dict added for debugging or additional info\n",
    "        info = {}\n",
    "\n",
    "        return self.grid.copy(), self.score, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            # Print the grid \n",
    "            for row in self.grid:\n",
    "                print(' '.join(str(cell).rjust(3) for cell in row))\n",
    "            print(f\"Agent Position: {self.agent_pos} | Score: {self.score}\\n\")\n",
    "\n",
    "\n",
    "class BallCatchingAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def act(self, state):\n",
    "        # Implement your agent's policy here\n",
    "        # For now, let's just return a random action\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "env = BallCatchingEnv(grid_size=(6, 12), max_balls=5)\n",
    "agent = BallCatchingAgent(env)\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What should be the initial setup of the environment?**\n",
    "\n",
    "- grid_size specifies the dimensions of the grid that represents the observation space. It's a 2D array where balls will \"fall\" from the top.\n",
    "- max_balls is the maximum number of balls that can be introduced in one step. It adds a level of variability to the game.\n",
    "- action_space is discrete, with three actions: stay in place, move left, or move right.\n",
    "- observation_space is a 2D array (Box space) representing the game grid. The values within this grid represent the absence of balls (0), the presence of balls (1 for now, but this can be - adjusted if different types of balls are introduced), and the agent (255 for clear differentiation).\n",
    "- agent_pos tracks the horizontal position of the agent, which moves only along the bottom row of the grid.\n",
    "- score and caught_balls track the game's progress and performance of the agent.\n",
    "- grid is the representation of the current state of the environment, including the positions of the agent and balls.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**What will the environment dynamics?**\n",
    "\n",
    "1. Ball Generation:\n",
    "    -  Balls (red and blue) are generated randomly at the top of the grid.\n",
    "    -  Logic to initialize balls at the top row with random column positions.\n",
    "    - The number of balls introduced at each step can vary from 1 to max_balls.\n",
    "2. Scoring:\n",
    "    - Catching a red ball awards 10 points.\n",
    "    - Catching a blue ball awards 20 points.\n",
    "3. Agent Movement:\n",
    "    - Action = 0: The agent stays in its current position.\n",
    "    - Action = 1: The agent moves left by one column (if not at the left edge).\n",
    "    - Action = 2: The agent moves right by one column (if not at the right edge).\n",
    "4. Ball Movement:\n",
    "    - Balls fall down by one row at each timestep.\n",
    "    - If a ball reaches the bottom row, check if it's in the same column as the agent. If so, the ball is considered caught, and points are awarded based on the ball's color.\n",
    "5. Edge Cases for Agent Movement:\n",
    "    -  If the agent is at the leftmost or rightmost position, actions to move further left or right should have no effect.\n",
    "6. Ball Tracking:\n",
    "    - Keep track of all balls currently falling. This can be done with a list of tuples or objects that store each ball's position and type.\n",
    "    - Red ball is represented by 1 and Blue ball is represented by 2. No ball is represented by 0. \n",
    "    - When balls move or are caught/missed, this list needs to be updated accordingly.\n",
    "7. Grid Representation Update:\n",
    "    - After each action and ball movement, the grid representation needs to be updated to reflect the current state, including the agent's new position and the positions of all balls.\n",
    "8. Episode Termination:\n",
    "    - An episode ends when the agent catches at least 5 balls, after which a new episode starts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from typing import Tuple\n",
    "\n",
    "class BallCatchingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size: Tuple[int, int]=(5, 12), max_balls: int=7):\n",
    "        super(BallCatchingEnv, self).__init__()\n",
    "        self.grid_height, self.grid_width = grid_size\n",
    "        self.max_balls = max_balls\n",
    "\n",
    "        # Actions: 0 = Stay, 1 = Left, 2 = Right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # 0 for empty, 1 for ball, and 255 for the agent\n",
    "        self.observation_space = spaces.Box(low=0, high=255, \n",
    "                                            shape=(self.grid_height, self.grid_width), \n",
    "                                            dtype=np.uint8)\n",
    "\n",
    "        self.agent_pos = self.grid_width // 2  # Start the agent in the middle of the grid\n",
    "        self.score = 0\n",
    "        self.caught_balls = 0\n",
    "        self.grid = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self.grid_width // 2\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "        self.balls = []  \n",
    "        self.score = 0\n",
    "        self.caught_balls = 0\n",
    "        self.grid[-1, self.agent_pos] = 255  # Use the bottom row for the agent\n",
    "        return self.grid.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        if action == 1 and self.agent_pos > 0:  # Move left\n",
    "            self.agent_pos -= 1\n",
    "        elif action == 2 and self.agent_pos < self.grid_width - 1:  # Move right\n",
    "            self.agent_pos += 1\n",
    "\n",
    "        # Generate new balls at the top of the grid\n",
    "        num_new_balls = np.random.randint(1, self.max_balls + 1)\n",
    "        for _ in range(num_new_balls):\n",
    "            ball_col = np.random.randint(0, self.grid_width)\n",
    "            ball_type = np.random.choice([1, 2])  # 1 for red, 2 for blue\n",
    "            self.balls.append([0, ball_col, ball_type])  # Append new ball at top row with random column\n",
    "\n",
    "\n",
    "        # Move existing balls down one row and check for catches\n",
    "        for ball in list(self.balls):  \n",
    "            ball[0] += 1  # Move ball down one row\n",
    "            if ball[0] == self.grid_height - 1:  \n",
    "                if ball[1] == self.agent_pos:  \n",
    "                    #print(f\"Catching ball of type: {ball[2]}, Current Score: {self.score}\") \n",
    "                    self.caught_balls += 1\n",
    "                    self.score += 20 if ball[2] == 2 else 10  \n",
    "                    #print(f\"New Score: {self.score}\") \n",
    "                # Remove the ball after processing, whether caught or missed\n",
    "                self.balls.remove(ball)\n",
    "        \n",
    "\n",
    "        # Update grid\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "        for ball in self.balls:\n",
    "            self.grid[ball[0], ball[1]] = ball[2]  \n",
    "        self.grid[self.grid_height - 1, self.agent_pos] = 255  \n",
    "\n",
    "\n",
    "        # episode termination\n",
    "        done = self.caught_balls >= 2\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.grid.copy(), self.score, done, info\n",
    "    \n",
    "    \n",
    "    def get_current_part_snapshot(self):\n",
    "        # Divide the grid width by 3 to define the width of each part\n",
    "        part_width = self.grid_width // 3\n",
    "\n",
    "        # Determine which part the agent is currently in\n",
    "        current_part = self.agent_pos // part_width\n",
    "\n",
    "        # Calculate the start and end indices of the current part\n",
    "        start_col = current_part * part_width\n",
    "        end_col = start_col + part_width\n",
    "\n",
    "        # Slice the grid to get the current part's snapshot\n",
    "        current_snapshot = self.grid[:, start_col:end_col]\n",
    "\n",
    "        return current_snapshot.copy(), current_part\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            # Print the grid \n",
    "            for row in self.grid:\n",
    "                print(' '.join(str(cell).rjust(3) for cell in row))\n",
    "            print(f\"Agent Position: {self.agent_pos} | Score: {self.score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallCatchingAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def calculate_steps_to_next_grid(self, agent_col, current_part_index):\n",
    "        part_width = self.env.grid_width // 3  \n",
    "        steps_to_left_border = agent_col - (current_part_index * part_width)  # Num of steps to the left\n",
    "        steps_to_right_border = ((current_part_index + 1) * part_width - 1) - agent_col  # Num of steps to the right\n",
    "\n",
    "        if current_part_index == 0:\n",
    "            # If grid 1, Go right\n",
    "            return steps_to_right_border + 1, 'R'\n",
    "        elif current_part_index == 1:\n",
    "            # If grid 2, return information about moving both sides\n",
    "            # steps_to_grid_0, direction_to_grid_0, steps_to_grid_1, direction_to_grid_1\n",
    "            return steps_to_left_border + 1, 'L', steps_to_right_border + 1, 'R'\n",
    "            #if steps_to_left_border < steps_to_right_border:\n",
    "            #    return steps_to_left_border + 1, 'L'\n",
    "            #else:\n",
    "            #    return steps_to_right_border + 1, 'R'\n",
    "        elif current_part_index == 2:\n",
    "            # If grid 3, move left \n",
    "            return steps_to_left_border + 1, 'L'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid grid part index\")\n",
    "        \n",
    "\n",
    "    def find_best_path_and_score(self, grid, start_row, start_col):\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "        directions = [(-1, 0, 'S'), (-1, -1, 'L'), (-1, 1, 'R')]\n",
    "        point_values = {1: 10, 2: 20, 255: 0} \n",
    "\n",
    "        def explore_path(row, col, path, score):\n",
    "            if row < 0 or col < 0 or col >= cols:\n",
    "                return (score, path)\n",
    "\n",
    "            current_score = point_values.get(grid[row][col], 0)\n",
    "            best_score = score + current_score\n",
    "            best_path = path\n",
    "\n",
    "            for d_row, d_col, action in directions:\n",
    "                next_row, next_col = row + d_row, col + d_col\n",
    "                if 0 <= next_row < rows and 0 <= next_col < cols:\n",
    "                    new_score, new_path = explore_path(next_row, next_col, path + [action], score + current_score)\n",
    "                    if new_score > best_score:\n",
    "                        best_score = new_score\n",
    "                        best_path = new_path\n",
    "            return (best_score, best_path)\n",
    "\n",
    "        total_score, best_path = explore_path(start_row, start_col, [], 0)\n",
    "        return best_path, total_score\n",
    "    \n",
    "\n",
    "    def find_agent_position(self, snapshot):\n",
    "        for col in range(len(snapshot[0])):\n",
    "            if snapshot[-1][col] == 255:  \n",
    "                return len(snapshot) - 1, col\n",
    "        return None  \n",
    "    \n",
    "    def calculate_points(self, grid, start_row, start_col, steps, direction):\n",
    "        point_values = {1: 10, 2: 20}\n",
    "        gained_points = 0\n",
    "        lost_points = 0\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "\n",
    "        \n",
    "  \n",
    "    def act(self,state):\n",
    "        # Get the current grid's snapshot\n",
    "        snapshot, current_part_index = self.env.get_current_part_snapshot()\n",
    "\n",
    "        agent_row, agent_col = self.find_agent_position(snapshot)\n",
    "\n",
    "        # Calculate the num of steps to the next grid \n",
    "        if current_part_index == 1:\n",
    "            steps_to_grid_left, direction_to_grid_left, steps_to_grid_right, direction_to_grid_right = self.calculate_steps_to_next_grid(agent_col, current_part_index)\n",
    "        else:\n",
    "            steps_to_next_grid, direction_to_next_grid = self.calculate_steps_to_next_grid(agent_col, current_part_index)\n",
    "\n",
    "        best_path, total_score = self.find_best_path_and_score(snapshot, agent_row, agent_col)\n",
    "\n",
    "        print(\"========================================================================\")\n",
    "        print(\"------------------ Snapshot---------------------------\")\n",
    "        print(snapshot)\n",
    "        print(\"-------------------Best Path --------------------------\")\n",
    "        print(best_path)\n",
    "        print(\"-------------------Score of this path---------------------------\")\n",
    "        print(total_score)\n",
    "        print(\"=============================== X ======================================\")\n",
    "        \n",
    "        # Decide the next action based on the first step in the best path\n",
    "        if best_path:\n",
    "            next_action = best_path[0]\n",
    "            if next_action == 'S':\n",
    "                return 0  # Stay\n",
    "            elif next_action == 'L':\n",
    "                return 1  # Left\n",
    "            elif next_action == 'R':\n",
    "                return 2  # Right\n",
    "        else:\n",
    "            # If for some reason the best_path is empty, take a random action\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "env = BallCatchingEnv(grid_size=(6, 12), max_balls=5)\n",
    "agent = BallCatchingAgent(env)\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "\n",
    "**Function for finding the best path TRIAL**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best path: ['S', 'L', 'R', 'R']\n",
      "Total score: 60\n"
     ]
    }
   ],
   "source": [
    "def find_best_path_and_score(grid, start_row, start_col):\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "    \n",
    "    # Directions the agent can move: (row_change, col_change, action)\n",
    "    directions = [(-1, 0, 'S'), (-1, -1, 'L'), (-1, 1, 'R')]\n",
    "    \n",
    "    # Map grid points to their scores\n",
    "    point_values = {1: 10, 2: 20, 255: 0}  # Agent's position does not add points\n",
    "    \n",
    "    def explore_path(row, col, path, score):\n",
    "        if row < 0 or col < 0 or col >= cols:\n",
    "            return (score, path)\n",
    "        \n",
    "        current_score = point_values.get(grid[row][col], 0)\n",
    "        best_score = score + current_score\n",
    "        best_path = path\n",
    "        \n",
    "        for d_row, d_col, action in directions:\n",
    "            next_row, next_col = row + d_row, col + d_col\n",
    "            if 0 <= next_row < rows and 0 <= next_col < cols:\n",
    "                new_score, new_path = explore_path(next_row, next_col, path + [action], score + current_score)\n",
    "                if new_score > best_score:\n",
    "                    best_score = new_score\n",
    "                    best_path = new_path\n",
    "        return (best_score, best_path)\n",
    "    \n",
    "    total_score, best_path = explore_path(start_row, start_col, [], 0)\n",
    "    return best_path, total_score\n",
    "\n",
    "grid = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 1, 2],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 0, 255, 0]\n",
    "]\n",
    "start_row, start_col = 5, 2\n",
    "\n",
    "best_path, total_score = find_best_path_and_score(grid, start_row, start_col)\n",
    "print(\"Best path:\", best_path)\n",
    "print(\"Total score:\", total_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Play**\n",
    "\n",
    "Feature Vector: The feature vector for training the neural network online should include:\n",
    "- Total potential score from the best path in the current grid.\n",
    "- Number of steps to the next grid.\n",
    "- Guaranteed points from moving towards the next grid.\n",
    "- Guaranteed losses from moving towards the next grid.\n",
    "- Current part index to provide context about the spatial location within the environment.\n",
    "\n",
    "Reward Function: The reward function should reflect the agent's performance in terms of both immediate and future rewards. Can consider including penalties for guaranteed losses and rewards for guaranteed points in the reward calculation to encourage strategic decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallCatchingAgent:\n",
    "    def __init__(self, env, neural_network):\n",
    "        self.env = env\n",
    "        self.neural_network = neural_network  # Assuming the NN is already trained and ready\n",
    "\n",
    "    def act(self):\n",
    "        snapshot, current_part_index = self.env.get_current_part_snapshot()\n",
    "        agent_row, agent_col = self.find_agent_position(snapshot)\n",
    "        best_path, potential_points = self.find_best_path_and_score(snapshot, agent_row, agent_col)\n",
    "\n",
    "        steps_to_next_grid = self.calculate_steps_to_next_grid(current_part_index, agent_col)\n",
    "\n",
    "        # NN input might include potential_points, steps_to_next_grid, current_part_index, etc.\n",
    "        nn_input = [potential_points, steps_to_next_grid, current_part_index]\n",
    "        move_to_next_grid = self.neural_network.predict(nn_input)  # Assuming a method to get NN decision\n",
    "\n",
    "        if move_to_next_grid:\n",
    "            # Decide direction based on current_part_index and potentially best_path\n",
    "            next_action = self.decide_movement_to_next_grid(current_part_index, best_path)\n",
    "        else:\n",
    "            # Follow the best immediate path\n",
    "            next_action = self.decide_action_from_best_path(best_path)\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    # Methods like calculate_steps_to_next_grid, decide_movement_to_next_grid, decide_action_from_best_path would need to be implemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guaranteed points gained: 10\n",
      "Guaranteed points lost: 40\n"
     ]
    }
   ],
   "source": [
    "def calculate_points(grid, start_row, start_col, steps, direction):\n",
    "    point_values = {1: 10, 2: 20}\n",
    "    gained_points = 0\n",
    "    lost_points = 0\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "\n",
    "    # Calculate gained points by moving left only\n",
    "    for step in range(1, steps + 1):\n",
    "        if direction == 'L':\n",
    "             val_adj = -step\n",
    "        elif direction == 'R':\n",
    "             val_adj = step\n",
    "        if start_col + val_adj >= 0: \n",
    "            cell_value = grid[start_row + val_adj][start_col + val_adj]\n",
    "            gained_points += point_values.get(cell_value, 0)\n",
    "    \n",
    "    # Calculate lost points by considering the positions directly above and to the right of the agent's path\n",
    "    for step in range(1, steps + 1):\n",
    "        row = start_row - step\n",
    "        max_vals = []\n",
    "        if step != 1:\n",
    "                start_col = start_col - step + 1\n",
    "        for col_diff in range(0, 2):  # Check the column of the agent, and one column to the right\n",
    "            col = start_col + col_diff\n",
    "            if 0 <= col < cols and 0 <= row:  \n",
    "                cell_value = grid[row][col]\n",
    "                max_vals.append(cell_value)\n",
    "\n",
    "        lost_points += point_values.get(max(max_vals) , 0)  \n",
    "        \n",
    "\n",
    "    return gained_points, lost_points\n",
    "\n",
    "# Example grid and starting position\n",
    "grid = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 1, 2],\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 2, 0, 0],\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 0, 255, 0]\n",
    "]\n",
    "start_row, start_col = 5, 2\n",
    "steps = 2  # Number of steps the agent will take to the left\n",
    "direction = 'L'\n",
    "gained_points, lost_points = calculate_points(grid, start_row, start_col, steps, direction)\n",
    "\n",
    "print(f\"Guaranteed points gained: {gained_points}\")\n",
    "print(f\"Guaranteed points lost: {lost_points}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step =  1\n",
      "GAINS :- Printing (row,col,points)\n",
      "4 0 0\n",
      "LOSS :- Printing (row, col, points)\n",
      "4 1 1\n",
      "4 0 0\n",
      "Step =  2\n",
      "GAINS :- Printing (row,col,points)\n",
      "LOSS :- Printing (row, col, points)\n",
      "3 0 0\n",
      "Direction: L\n",
      "Guaranteed points gained: 0\n",
      "Guaranteed points lost: 10\n"
     ]
    }
   ],
   "source": [
    "def calculate_points_directional(grid, start_row, start_col, steps, direction):\n",
    "    point_values = {1: 10, 2: 20}\n",
    "    gained_points = 0\n",
    "    lost_points = 0\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0])\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        print(\"Step = \",step)\n",
    "        # Adjust the direction of movement\n",
    "        if direction == 'L':\n",
    "            col_adjust = -step\n",
    "        elif direction == 'R':\n",
    "            col_adjust = step\n",
    "        else:\n",
    "            raise ValueError(\"Direction must be 'L' for left or 'R' for right\")\n",
    "\n",
    "        # Calculate gained points based on direction\n",
    "        print(\"GAINS :- Printing (row,col,points)\")\n",
    "        new_col = start_col + col_adjust\n",
    "        if 0 <= new_col < cols:\n",
    "            cell_value = grid[start_row - step][new_col]\n",
    "            print(start_row - step, new_col, cell_value)\n",
    "            gained_points += point_values.get(cell_value, 0)\n",
    "\n",
    "        # Calculate lost points by considering the positions above the agent\n",
    "        max_vals = []\n",
    "        # Adjust starting column based on the direction for each step\n",
    "        print(\"LOSS :- Printing (row, col, points)\")\n",
    "        adjust_col_for_lost = start_col if step == 1 else start_col + col_adjust + (1 if direction == 'L' else -1)\n",
    "        for col_diff in range(0, 2):  \n",
    "            col = adjust_col_for_lost + col_diff * (-1 if direction == 'L' else 1)\n",
    "            if 0 <= col < cols and 0 <= start_row - step:\n",
    "                cell_value = grid[start_row - step][col]\n",
    "                print(start_row - step,col,cell_value)\n",
    "                max_vals.append(cell_value)\n",
    "\n",
    "        if max_vals:\n",
    "            lost_points += point_values.get(max(max_vals), 0)\n",
    "\n",
    "    return gained_points, lost_points\n",
    "\n",
    "# Example usage:\n",
    "grid = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 0, 1, 2],\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 1, 2, 0],\n",
    "    [0, 0, 255, 0]\n",
    "]\n",
    "start_row, start_col = 5, 1\n",
    "steps = 2\n",
    "direction = 'L'  # 'L' for left, 'R' for right\n",
    "\n",
    "gained_points, lost_points = calculate_points_directional(grid, start_row, start_col, steps, direction)\n",
    "print(f\"Direction: {direction}\")\n",
    "print(f\"Guaranteed points gained: {gained_points}\")\n",
    "print(f\"Guaranteed points lost: {lost_points}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
